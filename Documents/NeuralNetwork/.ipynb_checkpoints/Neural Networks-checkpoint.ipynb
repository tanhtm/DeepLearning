{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "## 1. Definition\n",
    "\n",
    "<img src=\"images/deep_nn2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Notations\n",
    "\n",
    "- m : number of examples in the dataset\n",
    "- $n_x$ : input size\n",
    "- $n_y$ : output size (or number of classes)\n",
    "\n",
    "\n",
    "- $X \\in R^{n_x × m}$ is the input matrix\n",
    "- $x \\in R^{n_x × 1}$ is the input\n",
    "- $x^{(i)} \\in R^{n_x}$ is the $i^{th}$ example represented as a column vector\n",
    "- $x^{(i)}_j$ is the $j^{th}$ feature value of the $i^{th}$ traing example \n",
    "- $Y \\in R^{n_y × m}$ is the label matrix\n",
    "- $y^{(i)} \\in R^{n_y}$ is the output label for the $i^{th}$ example (**onehot**)\n",
    "- $w \\in R^{n_x × 1}$ is the weight\n",
    "- $b \\in R$ is the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. NN Architecture \n",
    "A NN is based on a collection of connected units or nodes. An unit neuron that receives a signal can process it and then signal additional artificial neurons connected to it.\n",
    "\n",
    "Signals travel from the first layer (the **input layer**), to the last layer (the **output layer**), possibly after traversing the **hidden layers** *multiple times*.\n",
    "<img src=\"images/architecture.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**: A NN has only 1 input and output layer but can have multiple hidden layers.\n",
    "<img src=\"images/type_nn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why do we need Deep Neural Networks ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/deep_nn.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Neuron Model (Logistic Unit)\n",
    "\n",
    "<img src=\"images/neuron.png\">\n",
    "\n",
    "Input unit: $x = \\left [\n",
    "\\begin{aligned}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_{n_x}\n",
    "\\end{aligned}\n",
    "\\right ]_{(n_x × 1 )}$\n",
    "Weight unit: $w = \\left [\n",
    "\\begin{aligned}\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "...\\\\\n",
    "w_{n_x}\n",
    "\\end{aligned}\n",
    "\\right ]_{(n_x × 1 )}$\n",
    "Bias unit: $b \\in \\mathbb{R}$\n",
    "\n",
    "$z = w^Tx + b$\n",
    "\n",
    "Activation unit: $a = \\sigma (z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Network Model (Set of Neurons)\n",
    "\n",
    "<img src=\"images/neural_net.jpeg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Notations:\n",
    "\n",
    "- $L$: Number of layers (Not count for imput layer).\n",
    "- $n^{[l]}$: Number of units in layer $l^{th}$, $n^{[0]} = n_x$, $0 \\leq l \\leq L$.\n",
    "- $\\sigma^{[l]}$ : Activation function of layer $l^{th}$\n",
    "- $a^{[l]}$: Activation in layer $l^{th}$, $a^{[0]} = x$.\n",
    "- $a^{[l]}_i$: Activation of unit $i^{[th]}$in layer $l^{th}$, $1 \\leq i \\leq n^{[l]}$.\n",
    "- $W^{[l]}$: Weight in layer $l^{th}$.\n",
    "- $w^{[l]}_i$: Weight of unit $i^{[th]}$ in layer $l^{th}$.\n",
    "- $b^{[l]}$: Bias in layer $l^{th}$.\n",
    "- $b^{[l]}_i$: Bias of unit $i^{[th]}$ in layer $l^{th}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.1. The unit $i^{th}$ in layer $l^{th}$\n",
    "Let: $\\large z^{[l]}_i = w^{[l]T}_i a^{[l-1]} + b^{[l]}_i \\in \\mathbb{R} $\n",
    "\n",
    "$\\large a^{[l]}_i = \\sigma^{[l]}{( w^{[l]T}_i a^{[l-1]} + b^{[l]}_i)} \\in \\mathbb{R}$\n",
    "\n",
    "With: \n",
    "\n",
    "$w^{[l]}_i = \\left [  \n",
    "\\begin{aligned} \n",
    "&w^{[l]}_{i,1}\\\\ \n",
    "&w^{[l]}_{i,2}\\\\\n",
    "&...\\\\\n",
    "&w^{[l]}_{i,n^{[l-1]}}\n",
    "\\end{aligned} \n",
    "\\right ] \\in \\mathbb{R}^{n^{[l-1]} × 1}$, \n",
    "$b^{[l]}_i \\in \\mathbb{R}$ and $a^{[l]} = \\left [  \n",
    "\\begin{aligned} \n",
    "&a^{[l]}_{1}\\\\ \n",
    "&a^{[l]}_{2}\\\\\n",
    "&...\\\\\n",
    "&a^{[l]}_{n^{[l]}}\n",
    "\\end{aligned} \n",
    "\\right ] \\in \\mathbb{R}^{n^{[l]} × 1}$\n",
    "\n",
    "$a^{[0]} = x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2. The layer $l^{th}$ for ONE example\n",
    "Let: $\\large z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\in \\mathbb{R}^{n^{[l]} × 1}$\n",
    "\n",
    "$\\large a^{[l]} = \\sigma{(z^{[l]})} \\in \\mathbb{R}^{n^{[l]} × 1}$\n",
    "\n",
    "With:\n",
    "\n",
    "$\n",
    "W^{[l]} = \\left [  \n",
    "\\begin{aligned} \n",
    "---&w^{[l]T}_1--- \\\\ \n",
    "---&w^{[l]T}_2--- \\\\\n",
    "&...\\\\\n",
    "---&w^{[l]T}_{n^{[l]}}---\n",
    "\\end{aligned} \\right ] \\in \\mathbb{R}^{n^{[l]} × n^{[l-1]}}$\n",
    "\n",
    "$b^{[l]} = \\left [  \n",
    "\\begin{aligned} \n",
    "&b^{[l]}_{1}\\\\ \n",
    "&b^{[l]}_{2}\\\\\n",
    "&...\\\\\n",
    "&b^{[l]}_{n^{[l-1]}}\n",
    "\\end{aligned} \n",
    "\\right ] \\in \\mathbb{R}^{n^{[l-1]} × 1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3. The layer $l^{th}$ for ALL examples\n",
    "\n",
    "Let: $\\large Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "$ \\large A^{[l]} = \\sigma(Z^{[l]})$\n",
    "\n",
    "Wiht: $A^{[l]} = \\left [ a^{[l](1)} a^{[l](2)} ... a^{[l](m)}\\right ] \\in \\mathbb{R}^{n^{l} × m}$\n",
    "\n",
    "$a^{[l](j)}$: Activation of layer $l^{th}$ is compute from example $j^{th}$\n",
    "\n",
    "$\\Longrightarrow A^{[0]} = X = \\left [ x^{(1)} x^{(2)} ... x^{(m)}\\right ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Multi-class Classification\n",
    "In order to make neural network to work with multi-class notification we may use **One-vs-All** approach.\n",
    "<img src=\"images/multi_class.png\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.1. Softmax\n",
    "$n^{[L]} = n_y$\n",
    "\n",
    "$Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}  \\in \\mathbb{R}^{n_y}$\n",
    "\n",
    "Activation function of last layer is **Softmax** funtion.\n",
    "\n",
    "$$\\large A^{[L]} = Softmax(Z^{[L]}) = 1\\backslash \\sum_{i=1}^{n_y} e^{z^{[L]}_i}$$\n",
    "\n",
    "$A^{[L]} \\in \\mathbb{R}^{n_y × m} $\n",
    "\n",
    "$a^{[L]}_i > 0  , 1 < i < n_y$\n",
    "\n",
    "$\\sum_{i=1}^{n_y} a^{[L]}_i  = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.2. Using One Hot encodings\n",
    "Many times in deep learning you will have a y vector with numbers ranging from 0 to C-1, where C is the number of classes. If C is for example 4, then you might have the following y vector which you will need to convert as follows:\n",
    "<img src=\"images/onehot.png\" style=\"width:600px;height:150px;\">\n",
    "This is called a \"one hot\" encoding, because in the converted representation exactly one element of each column is \"hot\" (meaning set to 1).\n",
    "\n",
    "After  encoding:\n",
    "\n",
    "$Y = [y^{(1)} ... y^{(m)}] \\in \\mathbb{R}^{n_y × m}$ s the label matrix (**Onehot**)\n",
    "\n",
    "$y^{(i)} \\in \\mathbb{R}^{n_y}$ is the output label for the $i^{th}$ example\n",
    "\n",
    "$y^{(i)}_j = 1$ if label of $i^{th}$ example is $j$ ($0 \\leq j < n_y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Cost function\n",
    "The cost function for the neuron network is quite similar to the logistic regression cost function.\n",
    "\n",
    "Activation function of last layer is **Softmax** function.\n",
    "\n",
    "Output is **OneHot**\n",
    "\n",
    "$\\large J\\left(W^{[1]}, b^{[1]}, ..., W^{[L]}, b^{[L]}\\right) \n",
    "= \\frac{-1}{m} \\sum_{i=1}^m \\sum_{j=1}^{n_y} y^{(i)}_j * \\log {a^{[L](i)}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
